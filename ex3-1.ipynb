{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5a24dc",
   "metadata": {},
   "source": [
    "# Aufgabe 1 Nochmal Rosenbrock-Funktion.\n",
    "\n",
    "Gegeben sei nochmals die Rosenbrockfunktion f (x, y) = 100(y − x2)2 + (1 − x)2. Sie d ¨urfen\n",
    "Ihre Implementierung von Aufgabenblatt 1 verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd03900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from commons import rosenbrock_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643de014",
   "metadata": {},
   "source": [
    "# c) Hesse Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2aa19f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 802, -400],\n",
       "       [-400,  200]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b) Hessematrix an (x, y)\n",
    "def rosenbrock_hesse(x, y):\n",
    "    return np.array([[-400 + 1200 * x * x + 2, -400 * x],[-400 * x, 200]])\n",
    "\n",
    "rosenbrock_hesse(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb75069",
   "metadata": {},
   "source": [
    "# d) Gradient Rosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a04d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.657305063079808), np.float64(0.43912711684196565))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c)\n",
    "\n",
    "def descent_gradiently(gradientf, x, y, iterations, dstep) -> tuple[float, float]:\n",
    "    for i in range(iterations):\n",
    "        gd = gradientf(x, y)\n",
    "        \n",
    "        d = -gd / np.linalg.norm(gd)\n",
    "        assert (d @ gd.T) < 0, \"Expected negative scalar, received \" + (d @ gd)\n",
    "        assert np.isclose(np.linalg.norm(d), 1), np.linalg.norm(d)\n",
    "        \n",
    "        dxy = d * dstep\n",
    "        xy_new = np.array([x, y]) + dxy\n",
    "        \n",
    "        # print(f\"[Iteration {i + 1}] [{x:.2f}, {y:.2f}] + {str(dxy)} = {xy_new:}\")\n",
    "        \n",
    "        x, y = xy_new\n",
    "    return x, y\n",
    "\n",
    "descent_gradiently(rosenbrock_gradient, 0.5, 0.5, 50, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f100b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def crosstest(func: callable, *varrs) -> None:\n",
    "    \"\"\"Test function with all combinations (product) of variables as arguments\"\"\"\n",
    "    for args in product(*varrs):\n",
    "        func(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c8417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations=     10\tdstep=     10\tx=( -2.969,   4.491)\n",
      "iterations=     10\tdstep=      5\tx=( -1.657,   4.311)\n",
      "iterations=     10\tdstep=      3\tx=( -1.258,   3.191)\n",
      "iterations=     10\tdstep=      1\tx=(  0.610,  -0.430)\n",
      "iterations=     10\tdstep=    0.1\tx=(  0.643,   0.359)\n",
      "iterations=     10\tdstep=   0.01\tx=(  0.574,   0.432)\n",
      "iterations=     10\tdstep=  0.001\tx=(  0.507,   0.493)\n",
      "iterations=     10\tdstep= 0.0001\tx=(  0.501,   0.499)\n",
      "iterations=    100\tdstep=     10\tx=( -1.561,   3.424)\n",
      "iterations=    100\tdstep=      5\tx=( -1.615,   3.931)\n",
      "iterations=    100\tdstep=      3\tx=(  1.254,  -0.822)\n",
      "iterations=    100\tdstep=      1\tx=(  0.181,   0.007)\n",
      "iterations=    100\tdstep=    0.1\tx=(  0.601,   0.311)\n",
      "iterations=    100\tdstep=   0.01\tx=(  0.696,   0.491)\n",
      "iterations=    100\tdstep=  0.001\tx=(  0.574,   0.433)\n",
      "iterations=    100\tdstep= 0.0001\tx=(  0.507,   0.493)\n",
      "iterations=   1000\tdstep=     10\tx=( -1.936,   5.239)\n",
      "iterations=   1000\tdstep=      5\tx=( -2.228,   6.199)\n",
      "iterations=   1000\tdstep=      3\tx=(  0.988,   2.362)\n",
      "iterations=   1000\tdstep=      1\tx=(  0.227,  -0.059)\n",
      "iterations=   1000\tdstep=    0.1\tx=(  0.566,   0.273)\n",
      "iterations=   1000\tdstep=   0.01\tx=(  0.905,   0.829)\n",
      "iterations=   1000\tdstep=  0.001\tx=(  0.897,   0.804)\n",
      "iterations=   1000\tdstep= 0.0001\tx=(  0.574,   0.433)\n",
      "iterations=  10000\tdstep=     10\tx=( -1.180,   0.621)\n",
      "iterations=  10000\tdstep=      5\tx=( -3.411,   0.389)\n",
      "iterations=  10000\tdstep=      3\tx=(  0.443,   0.454)\n",
      "iterations=  10000\tdstep=      1\tx=(  0.079,  -0.947)\n",
      "iterations=  10000\tdstep=    0.1\tx=(  0.566,   0.273)\n",
      "iterations=  10000\tdstep=   0.01\tx=(  0.986,   0.982)\n",
      "iterations=  10000\tdstep=  0.001\tx=(  0.999,   1.000)\n",
      "iterations=  10000\tdstep= 0.0001\tx=(  0.993,   0.986)\n"
     ]
    }
   ],
   "source": [
    "def rosenbrock_gd_wrapped(iterations, dstep):\n",
    "    x, y = descent_gradiently(rosenbrock_gradient, 0.5, 0.5, iterations, dstep)\n",
    "    print(f\"{iterations=:>7}\\t{dstep=:7}\\tx=({x:>7.3f}, {y:>7.3f})\")\n",
    "\n",
    "crosstest(rosenbrock_gd_wrapped, [10, 100, 1000, 10000], [10, 5, 3, 1, 0.1, 0.01, 0.001, 0.0001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564cff7",
   "metadata": {},
   "source": [
    "# d) Newton Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f106b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.32196047164231173), np.float64(0.10364416106304651))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d)    \n",
    "def descent_newtonianly(gradientf, hessef, x, y, iterations, dstep) -> tuple[float, float]:\n",
    "    for i in range(iterations):\n",
    "        gd = gradientf(x, y)\n",
    "        h = hessef(x, y)\n",
    "        d = - np.linalg.inv(h) @ gd.T\n",
    "        assert np.isclose(h @ d, -gd).all()\n",
    "        \n",
    "        xy_new = np.array([x, y]) + dstep * d\n",
    "        \n",
    "        x, y = xy_new\n",
    "    return x, y\n",
    "\n",
    "descent_newtonianly(rosenbrock_gradient, rosenbrock_hesse, 0.5, 0.5, 50, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c1db32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton optimization\n",
      "iterations=     10\tdstep=     10\tx=( -2.969,   4.491)\n",
      "iterations=     10\tdstep=      5\tx=( -1.657,   4.311)\n",
      "iterations=     10\tdstep=      1\tx=(  0.610,  -0.430)\n",
      "iterations=     10\tdstep=    0.1\tx=(  0.643,   0.359)\n",
      "iterations=     10\tdstep=   0.01\tx=(  0.574,   0.432)\n",
      "iterations=     10\tdstep=  0.001\tx=(  0.507,   0.493)\n",
      "iterations=     10\tdstep= 0.0001\tx=(  0.501,   0.499)\n",
      "iterations=    100\tdstep=     10\tx=( -1.561,   3.424)\n",
      "iterations=    100\tdstep=      5\tx=( -1.615,   3.931)\n",
      "iterations=    100\tdstep=      1\tx=(  0.181,   0.007)\n",
      "iterations=    100\tdstep=    0.1\tx=(  0.601,   0.311)\n",
      "iterations=    100\tdstep=   0.01\tx=(  0.696,   0.491)\n",
      "iterations=    100\tdstep=  0.001\tx=(  0.574,   0.433)\n",
      "iterations=    100\tdstep= 0.0001\tx=(  0.507,   0.493)\n",
      "iterations=   1000\tdstep=     10\tx=( -1.936,   5.239)\n",
      "iterations=   1000\tdstep=      5\tx=( -2.228,   6.199)\n",
      "iterations=   1000\tdstep=      1\tx=(  0.227,  -0.059)\n",
      "iterations=   1000\tdstep=    0.1\tx=(  0.566,   0.273)\n",
      "iterations=   1000\tdstep=   0.01\tx=(  0.905,   0.829)\n",
      "iterations=   1000\tdstep=  0.001\tx=(  0.897,   0.804)\n",
      "iterations=   1000\tdstep= 0.0001\tx=(  0.574,   0.433)\n",
      "iterations=  10000\tdstep=     10\tx=( -1.180,   0.621)\n",
      "iterations=  10000\tdstep=      5\tx=( -3.411,   0.389)\n",
      "iterations=  10000\tdstep=      1\tx=(  0.079,  -0.947)\n",
      "iterations=  10000\tdstep=    0.1\tx=(  0.566,   0.273)\n",
      "iterations=  10000\tdstep=   0.01\tx=(  0.986,   0.982)\n",
      "iterations=  10000\tdstep=  0.001\tx=(  0.999,   1.000)\n",
      "iterations=  10000\tdstep= 0.0001\tx=(  0.993,   0.986)\n"
     ]
    }
   ],
   "source": [
    "def rosenbrock_newton_wrapped(iterations, dstep):\n",
    "    x, y = descent_newtonianly(rosenbrock_gradient,rosenbrock_hesse, 0.5, 0.5, iterations, dstep)\n",
    "    print(f\"{iterations=:>7}\\t{dstep=:7}\\tx=({x:>7.3f}, {y:>7.3f})\")\n",
    "\n",
    "print(\"Newton optimization\")\n",
    "\n",
    "crosstest(rosenbrock_gd_wrapped, [10, 100, 1000, 10000], [10, 5, 1, 0.1, 0.01, 0.001, 0.0001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec953ca",
   "metadata": {},
   "source": [
    "# e) Levenberg-Marquart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d82df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d876eba",
   "metadata": {},
   "source": [
    "# f) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74813d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
